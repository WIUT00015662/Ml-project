{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ab5e61b",
   "metadata": {},
   "source": [
    "Loading the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ab6641f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = './data/883eax2-sup-0002.xlsx'\n",
    "df = pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83bfac6",
   "metadata": {},
   "source": [
    "Exploration of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a97dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Basic info\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"Column types:\\n\", df.dtypes)\n",
    "print(\"First 5 rows:\\n\", df.head())\n",
    "# 2. Summary statistics\n",
    "summary_stats = df.describe(include='all')  # include='all' for non-numeric too\n",
    "print(\"Summary statistics:\\n\", summary_stats)\n",
    "\n",
    "# 3. Check missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing values per column:\\n\", missing_values)\n",
    "\n",
    "# 4. Correlation matrix for numeric variables\n",
    "numeric_cols = df.select_dtypes(include=np.number).columns\n",
    "corr_matrix = df[numeric_cols].corr()\n",
    "print(\"Correlation matrix:\\n\", corr_matrix)\n",
    "\n",
    "# 5. Visualizations\n",
    "# Histogram for distributions\n",
    "df[numeric_cols].hist(figsize=(15,10), bins=20)\n",
    "plt.suptitle(\"Numeric Feature Distributions\")\n",
    "plt.show()\n",
    "\n",
    "# Boxplots to detect outliers\n",
    "plt.figure(figsize=(15,6))\n",
    "sns.boxplot(data=df[numeric_cols])\n",
    "plt.title(\"Boxplots of Numeric Features\")\n",
    "plt.show()\n",
    "\n",
    "# Heatmap of correlations\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Categorical columns\n",
    "cat_cols = df.select_dtypes(include='object').columns\n",
    "\n",
    "# Frequency counts for categorical columns\n",
    "for col in cat_cols:\n",
    "    print(f\"\\nColumn: {col}\")\n",
    "    print(df[col].value_counts().head(10))  # top 10 categories\n",
    "\n",
    "# Bar plots for some categorical features\n",
    "plt.figure(figsize=(12,5))\n",
    "sns.countplot(y='Activity', data=df, order=df['Activity'].value_counts().index)\n",
    "plt.title('Distribution of Activity')\n",
    "plt.show()\n",
    "\n",
    "# Distribution of target\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.histplot(df['Winter_mortality'], bins=20, kde=True)\n",
    "plt.title('Winter Mortality Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3777a6c",
   "metadata": {},
   "source": [
    "Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a0a00ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_like_cols = ['Age', 'Beekeep_for', 'Bee_population_size', 'Apiary_Size',\n",
    "                 'Swarm_bought', 'Swarm_produced', 'Queen_bought', 'Queen_produced']\n",
    "binary_cols = ['Qualif','Training','Coop_treat','Apiarist_book','Org_member','Continue',\n",
    "               'Chronic_Depop','ClinSign_Brood','ClinSign_Honeybees','H_Rate_ColMortality',\n",
    "               'H_Rate_HoneyMortality','OtherEvent','VarroaMites','QueenProblems',\n",
    "               'VarroosisV1','ChronicParalysisV1','AmericanFoulbroodV1','NosemosisV1',\n",
    "               'EuropeanFoulbroodV1','Migration','Merger']\n",
    "categorical_cols = ['Activity','Country','Production','Breed','Management',\n",
    "                    'MidSeason_Target','Environment','Program']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9399ff0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['ID_api'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf86f7ed",
   "metadata": {},
   "source": [
    "data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb3bfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "\n",
    "\n",
    "# 2. Handle missing values\n",
    "df['Environment'] = df['Environment'].fillna(df['Environment'].mode()[0])\n",
    "\n",
    "# check if remaining missing values are left\n",
    "print(\"remaining missing values\", df.isnull().sum())\n",
    "\n",
    "\n",
    "# we are trying to extract the numeric part before any delimiters\n",
    "for col in num_like_cols:\n",
    "    df[col] = df[col].apply(lambda x: int(str(x).split('___')[0].split('__')[0])) # example: \"1___Less than 30\" becomes 1\n",
    "\n",
    "\n",
    "for col in binary_cols:\n",
    "    df[col] = df[col].map({'Yes':1,'No':0,'Suffering':1,'Not_Suffering':0})\n",
    "\n",
    "\n",
    "# Quick look at transformed columns\n",
    "df[num_like_cols + binary_cols].head(10)\n",
    "for col in num_like_cols + binary_cols:\n",
    "    print(col, df[col].unique())\n",
    "\n",
    "print(df[num_like_cols + binary_cols].isnull().sum())\n",
    "\n",
    "df[num_like_cols].describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ae251049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric columns: ['Age', 'Beekeep_for', 'Qualif', 'Training', 'Coop_treat', 'Bee_population_size', 'Apiary_Size', 'Apiarist_book', 'Org_member', 'Continue', 'Chronic_Depop', 'ClinSign_Brood', 'ClinSign_Honeybees', 'H_Rate_ColMortality', 'H_Rate_HoneyMortality', 'OtherEvent', 'VarroaMites', 'QueenProblems', 'Swarm_bought', 'Swarm_produced', 'Queen_bought', 'Queen_produced', 'VarroosisV1', 'ChronicParalysisV1', 'AmericanFoulbroodV1', 'NosemosisV1', 'EuropeanFoulbroodV1', 'Migration', 'Merger']\n"
     ]
    }
   ],
   "source": [
    "numeric_cols = [col for col in df.columns if col not in categorical_cols + ['Winter_mortality']]\n",
    "print(\"Numeric columns:\", numeric_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2bf0dd",
   "metadata": {},
   "source": [
    "split/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da945667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric columns: ['Age', 'Beekeep_for', 'Qualif', 'Training', 'Coop_treat', 'Bee_population_size', 'Apiary_Size', 'Apiarist_book', 'Org_member', 'Continue', 'Chronic_Depop', 'ClinSign_Brood', 'ClinSign_Honeybees', 'H_Rate_ColMortality', 'H_Rate_HoneyMortality', 'OtherEvent', 'VarroaMites', 'QueenProblems', 'Swarm_bought', 'Swarm_produced', 'Queen_bought', 'Queen_produced', 'VarroosisV1', 'ChronicParalysisV1', 'AmericanFoulbroodV1', 'NosemosisV1', 'EuropeanFoulbroodV1', 'Migration', 'Merger']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "numeric_cols = [col for col in df.columns if col not in categorical_cols + ['Winter_mortality']]\n",
    "print(\"Numeric columns:\", numeric_cols)\n",
    "# 6. Split data into features and target\n",
    "X = df.drop(columns=['Winter_mortality'])\n",
    "y = df['Winter_mortality']\n",
    "\n",
    "# 7. Train/Validation/Test split (70/15/15)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ordinal_cols = ['Age','Beekeep_for','Bee_population_size','Apiary_Size',\n",
    "                'Swarm_bought','Swarm_produced','Queen_bought','Queen_produced']\n",
    "categorical_cols = ['Activity','Country','Production','Breed','Management',\n",
    "                    'MidSeason_Target','Environment','Program']\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Numeric/ordinal pipeline: impute missing values with mean, then scale\n",
    "numeric_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical pipeline: impute missing values with most frequent, then one-hot encode\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(drop='first', handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# ColumnTransformer with pipelines\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_pipeline, ordinal_cols),\n",
    "    ('cat', categorical_pipeline, categorical_cols)\n",
    "], remainder='passthrough')  # binary columns remain unchanged\n",
    "\n",
    "# Apply transformation\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_val_processed = preprocessor.transform(X_val)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "47c37710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qualif [0 1]\n",
      "Training [1 0]\n",
      "Coop_treat [1 0]\n",
      "Apiarist_book [1 0]\n",
      "Org_member [1 0]\n",
      "Continue [1 0]\n",
      "Chronic_Depop [0 1]\n",
      "ClinSign_Brood [0 1]\n",
      "ClinSign_Honeybees [0 1]\n",
      "H_Rate_ColMortality [0 1]\n",
      "H_Rate_HoneyMortality [0 1]\n",
      "OtherEvent [0 1]\n",
      "VarroaMites [1 0]\n",
      "QueenProblems [0 1]\n",
      "VarroosisV1 [0 1]\n",
      "ChronicParalysisV1 [0 1]\n",
      "AmericanFoulbroodV1 [0 1]\n",
      "NosemosisV1 [0 1]\n",
      "EuropeanFoulbroodV1 [0 1]\n",
      "Migration [0 1]\n",
      "Merger [0 1]\n"
     ]
    }
   ],
   "source": [
    "for col in binary_cols:\n",
    "    print(col, df[col].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0da75e1",
   "metadata": {},
   "source": [
    "Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08118973",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(\"NaNs in X_train:\")\n",
    "print(pd.DataFrame(X_train).isnull().sum())\n",
    "\n",
    "print(\"NaNs in ordinal columns:\")\n",
    "print(X_train[ordinal_cols].isnull().sum())\n",
    "\n",
    "print(\"NaNs in categorical columns:\")\n",
    "print(X_train[categorical_cols].isnull().sum())\n",
    "\n",
    "print(\"NaNs in remaining columns (binary, passthrough):\")\n",
    "remaining_cols = [c for c in X_train.columns if c not in ordinal_cols + categorical_cols]\n",
    "print(X_train[remaining_cols].isnull().sum())\n",
    "\n",
    "# Helper function to evaluate\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    print(f\"--- {model_name} ---\")\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))  # safe for any sklearn version\n",
    "    print(\"RMSE:\", rmse)\n",
    "    print(\"MAE:\", mean_absolute_error(y_true, y_pred))\n",
    "    print(\"R^2:\", r2_score(y_true, y_pred))\n",
    "    print(\"\\n\")\n",
    "\n",
    "# 1. Linear Regression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_processed, y_train)\n",
    "y_val_pred_lr = lr.predict(X_val_processed)\n",
    "evaluate_model(y_val, y_val_pred_lr, \"Linear Regression\")\n",
    "\n",
    "# 2. Random Forest Regressor\n",
    "rf = RandomForestRegressor(n_estimators=200, max_depth=None, random_state=42)\n",
    "rf.fit(X_train_processed, y_train)\n",
    "y_val_pred_rf = rf.predict(X_val_processed)\n",
    "evaluate_model(y_val, y_val_pred_rf, \"Random Forest\")\n",
    "\n",
    "# 3. Gradient Boosting Regressor\n",
    "gb = GradientBoostingRegressor(n_estimators=200, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "gb.fit(X_train_processed, y_train)\n",
    "y_val_pred_gb = gb.predict(X_val_processed)\n",
    "evaluate_model(y_val, y_val_pred_gb, \"Gradient Boosting\")\n",
    "\n",
    "# --- Linear Regression ---\n",
    "# RMSE: 18.747355794878878\n",
    "# MAE: 12.670191530342345\n",
    "# R^2: 0.1476944466416199\n",
    "\n",
    "\n",
    "# --- Random Forest ---\n",
    "# RMSE: 19.576388615537716\n",
    "# MAE: 13.036145337483344\n",
    "# R^2: 0.07064758839180507\n",
    "\n",
    "\n",
    "# --- Gradient Boosting ---\n",
    "# RMSE: 18.868034280775355\n",
    "# MAE: 12.632342875715272\n",
    "# R^2: 0.13668638894169238"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841baf3a",
   "metadata": {},
   "source": [
    "Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81aca7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 216 candidates, totalling 648 fits\n",
      "Best RF parameters: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200}\n",
      "Best RF score (neg MSE): -334.0638319389099\n",
      "Fitting 3 folds for each of 324 candidates, totalling 972 fits\n",
      "Best GB parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Best GB score (neg MSE): -331.01559885219007\n",
      "--- Tuned Random Forest ---\n",
      "RMSE: 18.661424077849787\n",
      "MAE: 12.541636337983208\n",
      "R^2: 0.15548991670223877\n",
      "\n",
      "\n",
      "--- Tuned Gradient Boosting ---\n",
      "RMSE: 18.667019550230183\n",
      "MAE: 12.541718292453268\n",
      "R^2: 0.15498340216538942\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "rf_params = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 5, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "rf_grid = GridSearchCV(\n",
    "    RandomForestRegressor(random_state=42),\n",
    "    param_grid=rf_params,\n",
    "    scoring='neg_mean_squared_error',   # since we want low RMSE\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "rf_grid.fit(X_train_processed, y_train)\n",
    "\n",
    "print(\"Best RF parameters:\", rf_grid.best_params_)\n",
    "print(\"Best RF score (neg MSE):\", rf_grid.best_score_)\n",
    "\n",
    "\n",
    "gb_params = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'max_depth': [2, 3, 5],\n",
    "    'subsample': [0.7, 0.9, 1.0],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "gb_grid = GridSearchCV(\n",
    "    GradientBoostingRegressor(random_state=42),\n",
    "    param_grid=gb_params,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "gb_grid.fit(X_train_processed, y_train)\n",
    "\n",
    "print(\"Best GB parameters:\", gb_grid.best_params_)\n",
    "print(\"Best GB score (neg MSE):\", gb_grid.best_score_)\n",
    "\n",
    "\n",
    "# Random Forest tuned\n",
    "rf_best = rf_grid.best_estimator_\n",
    "rf_val_pred = rf_best.predict(X_val_processed)\n",
    "evaluate_model(y_val, rf_val_pred, \"Tuned Random Forest\")\n",
    "\n",
    "# Gradient Boosting tuned\n",
    "gb_best = gb_grid.best_estimator_\n",
    "gb_val_pred = gb_best.predict(X_val_processed)\n",
    "evaluate_model(y_val, gb_val_pred, \"Tuned Gradient Boosting\")\n",
    "\n",
    "\n",
    "# Fitting 3 folds for each of 216 candidates, totalling 648 fits\n",
    "# Best RF parameters: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200}\n",
    "# Best RF score (neg MSE): -334.0638319389099\n",
    "# Fitting 3 folds for each of 324 candidates, totalling 972 fits\n",
    "# Best GB parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 200, 'subsample': 0.9}\n",
    "# Best GB score (neg MSE): -331.01559885219007\n",
    "# --- Tuned Random Forest ---\n",
    "# RMSE: 18.661424077849787\n",
    "# MAE: 12.541636337983208\n",
    "# R^2: 0.15548991670223877\n",
    "\n",
    "\n",
    "# --- Tuned Gradient Boosting ---\n",
    "# RMSE: 18.667019550230183\n",
    "# MAE: 12.541718292453268\n",
    "# R^2: 0.15498340216538942\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a99e000",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a149f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Final Tuned Gradient Boosting (Test Set) ---\n",
      "RMSE: 17.141968089635913\n",
      "MAE: 12.185823537424666\n",
      "R^2: 0.15733452302430928\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Final chosen model (Gradient Boosting)\n",
    "gb_best = gb_grid.best_estimator_\n",
    "y_test_pred = gb_best.predict(X_test_processed)\n",
    "\n",
    "evaluate_model(y_test, y_test_pred, \"Final Tuned Gradient Boosting (Test Set)\")\n",
    "# --- Final Tuned Gradient Boosting (Test Set) ---\n",
    "# RMSE: 17.141968089635913\n",
    "# MAE: 12.185823537424666\n",
    "# R^2: 0.15733452302430928"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
